{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Tuning\n",
    "==================\n",
    "\n",
    "In this notebook we consider performance tuning of parallel algorithms.  We use the nyc taxi data from the last exercise.  We also use the dask diagnostic dashboard during this exercise.  Now would be a good time to connect to it.  We recommend running the jupyter notebook and the dask diagnostic status page side by side.\n",
    "\n",
    "This notebook uses Dask.  You may want to use [Dask's diagnostic dashboard](../../../9002/status) while running this notebook for feedback from the cluster.  We recommend setting up the dashboard and your notebook side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress, wait\n",
    "\n",
    "client = Client('schedulers:9000')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv('gcs://anaconda-public-data/nyc-taxi/csv/2015/yellow_tripdata_2015-*.csv',\n",
    "                 parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                 storage_options={'token': 'cloud'})\n",
    "df = df.persist()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "progress(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition size\n",
    "\n",
    "Performance in distributed systems depends on many factors.  Some of these are familiar from single-machine computing but some are new.  In this section we consider the costs of too many or too few partitions.\n",
    "\n",
    "In the following lines we split our dask.dataframe into 1000 smaller pandas dataframes.  This is both good and bad:\n",
    "\n",
    "1. **Good**:  It exposes more parallelism.  If we have more cores we can split the computation more finely.\n",
    "2.  **Bad**: It adds more overhead.  There is a fixed cost to every task.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Run the code below and use Dask's diagnostic dashboard to investigate what is taking up time.  Change two parameters in the computation to make the second cell as fast as possible:\n",
    "\n",
    "-  `npartitions`: The number of partitions for our dataframe\n",
    "-  `split_every`: The granularity by which we reduce intermediate values in the sum\n",
    "\n",
    "*Note: lets not care about the cost of the first cell where we repartition.  This is typically done once at data ingestion.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.repartition(npartitions=1000).persist()\n",
    "wait(df2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df2.passenger_count.sum(split_every=1000).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication \n",
    "\n",
    "In this section we look at distributed matrix multiply.  This algorithm can be bound by communication depending on how the array is chunked.\n",
    "\n",
    "We make a distributed numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "x = x.persist()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform a matrix multiply of x by itself.  Watch the diagnostic dashboard, what do you notice?  In particular track the amount of *red* in the Task Stream plot, which corresponds to communication, and the amount of intermedaite data stored in the upper left per worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = x.dot(x.T).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change chunking\n",
    "\n",
    "Currently our array is stored as a 10x10 grid of 1000x1000 numpy arrays.  We can change the chunkshape using the `.reshape` method.  The chunk shape that we chose can strongly impact the cost of the matrix multiply algorithm.  \n",
    "\n",
    "We might choose larger or smaller chunks \n",
    "\n",
    "    x = x.rechunk((100, 100)).persist()  # more and smaller chunks\n",
    "    x = x.rechunk((2000, 2000)).persist()  # more and smaller chunks\n",
    "    \n",
    "Or we might choose chunks of a different size\n",
    "\n",
    "    x = x.rechunk((2000, 500)).persist()  # make chunks tall and skinny\n",
    "    x = x.rechunk((500, 2000)).persist()  # make chunks short and wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = x.rechunk((500, 500)).persist()\n",
    "wait(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "y = x.dot(x.T).persist()\n",
    "wait(y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
