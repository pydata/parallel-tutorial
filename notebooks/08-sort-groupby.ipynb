{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" align=\"right\">\n",
    "\n",
    "PySpark and Data Movement Costs\n",
    "=================\n",
    "\n",
    "We've seen how Big Data collections like the PySpark RDD provide parallel and distributed versions of common operations.  These allow us to write distributed code similar to how we write sequential code.  However, while these operations may produce the same result, they also have different costs from what we might be used to.  Some operations that were previously fast may now be very slow.  Some operations that were slow may now be fast.\n",
    "\n",
    "Fortunately there are often alternative algorithms to achieve the same results in faster time.  Understanding when to use these can greatly speed up our analyses.  In this notebook we look at two examples:\n",
    "\n",
    "1.  Finding the largest elements of a collection of random numbers\n",
    "2.  Performing a groupby-aggregate query on JSON records of GitHub data.\n",
    "\n",
    "In each example we consider the performance of both a straightforward-and-slow approach, as well as introduce a less-straightforward but much faster approach.\n",
    "\n",
    "*Note: there are expensive serialization costs moving from Python to JVM*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and TopK with Random data\n",
    "\n",
    "We create a large set of random numbers and store them as an RDD.  We find the largest numbers with two methods:\n",
    "\n",
    "1.  Sort the RDD, then take the top five elements\n",
    "2.  Call the `top` method\n",
    "\n",
    "We find that calling the specialized `top` method is *much* faster than performing a full sort.  \n",
    "\n",
    "*Note: had we used the spark dataframe API then Spark would have converted the first call into the second automatically.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, randn\n",
    "spark = SparkSession.builder.master('spark://schedulers:7077').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 10000000, numPartitions=4)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataset \n",
    "\n",
    "random_df = df.select(rand(seed=10).alias(\"uniform\"))\n",
    "rdd = random_df.rdd.map(lambda x: x[0]).cache()\n",
    "print(rdd.count())\n",
    "print(rdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time rdd.sortBy(lambda t: t, ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time rdd.top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... with DataFrames\n",
    "\n",
    "Spark dataframes are faster than Spark RDDs here for two reasons:\n",
    "\n",
    "1.  It can do high-level query optimizations to turn `sort+take` into `top`.\n",
    "2.  It can operate directly on efficient data structures rather than many small Python objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time random_df.sort('uniform', ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby-aggregate with Github JSON data\n",
    "\n",
    "We learn the same lesson, that smarter algorithms can be much faster than the obvious approach, this time with real data.  \n",
    "\n",
    "We read some JSON GitHub Data with Spark.  This includes every commit, comment, and pull request that occurred January 1st, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"s3a://githubarchive-data/2015-01-01-*.json.gz\")\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data into distributed memory\n",
    "dfc = df.cache()\n",
    "js = dfc.rdd\n",
    "print(js.count(), js.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of records, grouped by type\n",
    "\n",
    "### ... with groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "js.groupBy(lambda d: d['type']).map(lambda kv: (kv[0], len(kv[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... with combineByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def add(acc, x): return acc + 1\n",
    "def global_add(x, y): return x + y\n",
    "\n",
    "js.keyBy(lambda d: d['type']).combineByKey(lambda x: 1, add, global_add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "      <td>\n",
    "        <img src=\"https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/group_by.png\" width=\"400\">\n",
    "      </td>\n",
    "      <td>\n",
    "        <img src=\"https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/reduce_by.png\" width=\"400\">\n",
    "      </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "[--Databricks](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... with DataFrames\n",
    "\n",
    "Again, Spark dataframes let us use straightforward syntax `groupby(...).count()` but rewrites our intent to the more efficient approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time dfc.groupBy(dfc['type']).count().collect()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
