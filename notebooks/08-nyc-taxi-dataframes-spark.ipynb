{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\">\n",
    "\n",
    "## DataFrames (Spark)\n",
    "\n",
    "In the previous notebook we manipulated and queried a large parquet file by splitting it up into many Pandas dataframes with the concurrent.futures API.  This was straightforward, but required us to be clever to implement various parallel algorithms.  Fortunately, for certain classes of algorithms various big data projects have already done this work for us.  Dealing with tabular data like the data in our NYC-Taxi dataset is well covered by many systems like SQL databases, Spark, and Dask.dataframes.\n",
    "\n",
    "At the end of the last notebook we saw that we could use Pandas-like syntax to accomplish the same result with less programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('spark://schedulers:7077').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['tpep_pickup_datetime', 'passenger_count', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'tip_amount', 'total_amount']\n",
    "\n",
    "df = (spark.read.parquet('s3a://dask-data/nyc-taxi/nyc-2015.parquet')\n",
    "           .select(*columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg({'passenger_count': 'sum'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg({'passenger_count': 'avg'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('passenger_count').agg({'*': 'count'}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "In this notebook we will explore this API and this dataset further.  We will determine how well people tip based on the number of passengers in a cab.  To do this we will accomplish the following:\n",
    "\n",
    "1.  Remove rides with zero fare\n",
    "2.  Add a new column `tip_fraction` that is equal to the ratio of the tip to the fare\n",
    "3.  Group by the `passenger_count` column and take the mean of the `tip_fraction` column.\n",
    "\n",
    "You may want to refer to these resources to help you with the Spark DataFrame API\n",
    "\n",
    "-  https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "-  https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "And refer to the [Spark UI](../../../9070) for feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to remove rows\n",
    "\n",
    "In Spark you can filter rows by a boolean expression like the following:\n",
    "\n",
    "```python\n",
    "df = df.filter(df.name == 'Alice')\n",
    "```\n",
    "\n",
    "### How to make new columns\n",
    "\n",
    "In Pandas you can create a new column using Python's setitem syntax like the following:\n",
    "\n",
    "```python\n",
    "df = df.withColumn('z', df.x + df.y)\n",
    "```\n",
    "\n",
    "### How to do groupby-aggregations\n",
    "\n",
    "In Pandas you can do a groupby-aggregation by using the `groupby` method, followed by a column name an aggregation method like the following:\n",
    "\n",
    "```python\n",
    "df.groupBy(df.name).agg({'column-name': 'avg'})\n",
    "```\n",
    "\n",
    "When you want to collect the result of your computation, finish with the `.collect()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the operations above to find out how well New Yorkers tip based on the number of passengers in the cab.  Be sure to filter out rides with zero fare first to get a good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/nyc-taxi-dataframes-spark.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional exercises\n",
    "\n",
    "If you're already experienced with Pandas then we recommend also looking at the following questions:\n",
    "\n",
    "1. How well do New Yorkers tip as a function of the hour of day and the day of the week?\n",
    "2.  Investiate the `payment_type` column.  See how well each of the payment types correlate with the `tip_fraction`.  Did you find anything interesting?  Any guesses on what the different payment types might be?  If you're interested you may be able to find more information on the [NYC TLC's website](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml)\n",
    "3.  How quickly can you get the data for a particular day of the year?  How about for a particular hour of that day?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
